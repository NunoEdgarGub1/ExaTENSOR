ExaTENSOR development list (D.I.L.):
===================================

FEATURES NEEDED:
2018/09/28: Tensor addition with permutation and conjugation needs to be implemented in TAL-SH and ExaTENSOR:
            CP-TAL: tensor addition with permutation is missing;
            NV-TAL: tensor addition with conjugation is missing.
2018/09/28: Tensor contraction with conjugation needs to be implemented in NV-TAL.
2018/09/28: Tensor slicing/insertion needs to be implemented in TAL-SH and ExaTENSOR.
2018/10/26: exatns_tensor_create() must be able to create tensor slices.
2018/10/26: TAL-SH should introduce additional layers on top of eager API inferace:
            Lazy layer: Placing tasks into the global queue, decomposing too large tasks
            into smaller tasks, aggregating smaller tasks into task graphs, binding tasks
            to basic execution units based on task attributes and data locality, reusing
            tensor images between tasks (communication minimization), etc.


UNSOLVED ISSUES:
2018/03/07: Implement tensor R/W status update (in the cache) at the TAVP-MNG level:
            Dependency check and status update on instruction issue should be done during location cycle;
            Status update on instruction retirement will probably require another rotation cycle, or
            it may be implemented via remote MPI atomics.
2018/03/07: At the bottom TAVP-MNG level it is unclear how to implement replication within a group of the
            TAVP-WRK processes belonging to the same TAVP-MNG process. Perhaps argument renaming and
            replication under modified names provides an acceptable solution:
            T12(a,b,i,j) --> T12$0, T12$1, T12$2, etc. Note that later the output tensor name substitution
            may additionally mangle the name: T12 --> T12#0 (accumulator); T12#1, T12#2, etc. (temporary).
            Thus, combined will give: T12 (original) --> T12$1 (replica) --> T12$1#3 (temporary replica).
2018/03/26: TAL-SH coherence control might not work properly when the same tensor is concurrently participating
            in multiple tensor operations as an input. Needs to be checked.
2018/04/05: Universal memory allocator may now return an error code, but never TRY_LATER. The resource acquisition
            member procedure in tens_resrc_t should somehow separate cases of TRY_LATER:
            mem_allocate() -> tens_resrc_t.allocate_buffer() -> tens_oprnd_t.acquire_rsc() & tens_entry_wrk_t.acquire_resource() ->
            -> tavp_wrk_resourcer_t.acquire_resource().
2018/04/09: It is not clear when to upload and when to destroy a local accumulator tensor: TMP_COUNT never decreases.
2018/04/24: TAVP_INSTR_CTRL_STOP should not be reordered with other instructions in TAVP-WRK:
            It must reach each DSVU the last, that is, each DSVU must pass the STOP instruction as its last instruction.
2018/08/20: Temporary (output) tensors must be reused, especially if they are already on GPU.
2018/09/12: Tensor transformation requires MPI_PUT and MPI_GET for output tensor arguments:
            Get output tensor argument --> Transform it locally --> Put output tensor argument.
            Tensor initialization requires MPI_PUT. Tensor operations with INOUT operands also
            require MPI_GET + MPI_PUT mechanism on output operands.
2018/10/26: Memory per MPI process needs to be specified in an environment variable and then read by ExaTENSOR.
            Memory allocations/deallocations for tensors should be done via my custom memory allocator,
            with a fallback to regular allocations in case the custom allocator is out of memory space.
2018/10/26: Frequent small object allocations/deallocations should be replaced by GFC::bank factory.
2018/10/26: TAVP-MNG:Dispatcher should dispatch tensor instructions in bounded-size packets, thus
            achieving load balancing. For this, retired tensor instruction packets must contain
            a tag specifying which dispatch channel the packet came from.


SOLVED:
 2018/03/27: Subspace decomposition algorithm with alignment does not work properly: Example with alignment 5:
             [50] -> [[20]+[30]] -> [[[10]+[10]]+[[25]+[5]]]
 2014/07/07: Check GPU_UP before switching GPUs.
 2014/07/07: Add a scalar scaling factor to <tensor_block_contract> and <gpu_tensor_block_contract>.
 2014/01/04: Add concurrent kernel execution into tensor_block_contract_dlf_r4_ for input tensor transposes.
             Discarded. I do not think there would be any significant performance gain.
 2014/05/13: Use "restricted" pointers in .cu files.
 2014/01/02: gpu_tensor_block_copy_dlf_r4__ kernel does too many divisions/modulo. Solved via tables.
 2013/12/30: No overlap observed between two tensor contractions set to different CUDA streams:
             cudaEventRecord() calls lead to a serialization of the streams execution!
 2013/12/26: Seems like device pointers must be 64-bit aligned in order to get DMA work.
 2013/12/23: cudaHostAlloc() must use cudaHostAllocPortable flag!
 2013/12/19: GPU tensor transpose kernel still uses <int> instead of <size_t> for offsets. Fixed.
 2013/12/19: gpu_tensor_block_contract:
             (a) the inversed destination tensor transpose must have inversed dimension order!
             (b) cudaMemcpyToSymbolAsync will fail for permutations because they have local scope!
                 The permutation must be allocated in the packet and incorporated into tensBlck_t.
 2013/12/04: I need to create a CUDA task pool (cuda_task_create, cuda_task_destroy).
 2013/10/31: Efficient tensor transpose on GPU: Definition of the minor index set:
             (a) Minor input volume >= warpSize; (b) Minor output volume >= warpSize;
             (c) Minor volume <= shared memory buffer size (but as close as possible).
 2013/07/17: If minor index sets differ && the last minor dimension(s) is(are) very long, split it(them).
             Write a short paper on my algorithm for cache-efficient tensor transposes.
 2013/07/16: What should %scalar_value reflect when the tensor rank is higher than zero?
             How to make the %data_real consistent with %data_cmplx8? Complex --> Real conversion done!
