*** TAL-SH: Tensor Algebra Library for Shared Memory Computers ***
AUTHOR: Dmitry I. Lyakh (Liakh): quant4me@gmail.com, liakhdi@ornl.gov
REVISION: 2016/02/08
Copyright (C) 2015 Dmitry I. Lyakh
Copyright (C) 2015 Oak Ridge National Laboratory (UT-Battelle)
LICENSE: GPL v.2
--------------------------------------------------------------


0. Preamble

The TAL-SH library provides API for performing basic tensor algebra
operations on multicore CPU, NVidia GPU, Intel Xeon Phi, and other
accelerators (in future). Basic tensor algebra operations include
tensor contraction, tensor product, tensor addition, tensor transpose,
multiplication by a scalar, etc., which operate on locally stored tensors.
The execution of tensor operations on accelerators is asynchronous with
respect to the CPU Host, if the underlying node is heterogeneous. Both
Fortran and C/C++ API interfaces are provided (Fortran 2003 and later).


1. Nomenclature

Tensor: A multi-dimensional array of real or complex numbers.
Both real and complex numbers come either in a single-precision
form (R4/C4) or double precision form (R8/C8).

Tensor Block (TB): A locally stored slice of a tensor.
In general, a tensor block is defined by its
(a) Data type: Real (R4, R8) or complex (C4, C8);
(b) Signature: A unique identifier of this specific tensor block;
(c) Shape: Tensor block rank, dimension extents, storage layout;
(d) Tensor elements: Values of the elements of the tensor block.

Host Argument Buffer (HAB): An optional buffer can be created in
pinned Host memory that can be later used for storing tensor blocks.
Preallocation of this buffer is recommended in order to avoid
memory fragmentation issues and minimize the overhead associated
with Host memory pinning when running asynchronous operations.


2. Device control API

Supported device kinds:
 DEV_NULL: Abstract null device [-1];
 DEV_HOST: Multicore CPU: Device range [0];
 DEV_NVIDIA_GPU: NVidia GPU: Device range [0..MAX_GPUS_PER_NODE-1];
 DEV_INTEL_MIC: Intel Xeon Phi: Device range [0..MAX_MICS_PER_NODE-1];
 DEV_AMD_GPU: AMD GPU (future): Device range [0..MAX_AMDS_PER_NODE-1];

 Note that self-hosted platforms will fall under the DEV_HOST kind,
 regardless of the actual hardware.

Flat device numeration:
 Each device of each kind also has a flat device id in the range [0..DEV_MAX-1].
 0 is always reserved for the Host. Some functions, which convert
 between the flat and device kind specific numerations, may also return DEV_MAX
 with the meaning of "out of range" (error code). Hereafter, the id of a device
 within its kind will be referred to as "kind-specific", as opposed to its flat id.

API: Initialize the TAL-SH library:

 Fortran 2003:
  function talsh_init(host_buf_size,host_arg_max,gpu_list,mic_list,amd_list) result(ierr):
   integer(C_INT):: ierr                                      !out: error code (0:success)
   integer(C_SIZE_T), intent(inout), optional:: host_buf_size !inout: desired size in bytes of the Host Argument Buffer.
                                                              !       It will be replaced by the actual size.
   integer(C_INT), intent(out), optional:: host_arg_max       !out: max number of arguments the HAB can contain
   integer(C_INT), intent(in), optional:: gpu_list(1:)        !in: list of NVidia GPU's to use
   integer(C_INT), intent(in), optional:: mic_list(1:)        !in: list of Intel Xeon Phi's to use
   integer(C_INT), intent(in), optional:: amd_list(1:)        !in: list of AMD GPU's to use

 C/C++:
  int talshInit(           //out: error code (0:success)
   size_t * host_buf_size, //inout: desired size in bytes of the Host Argument Buffer.
                           //       It will be replaced by the actual size.
   int * host_arg_max,     //out: max number of arguments the HAB can contain
   int ngpus,              //in: number of NVidia GPUs to use
   int gpu_list[],         //in: list of NVidia GPUs to use
   int nmics,              //in: number of Intel Xeon Phi's to use
   int mic_list[],         //in: list of Intel Xeon Phi's to use
   int namds,              //in: number of AMD GPU's to use
   int amd_list[]          //in: list of AMD GPU's to use
  )

API: Shutdown the TAL-SH library:

 Fortran 2003:
  function talsh_shutdown() result(ierr):
   integer(C_INT):: ierr !out: error code (0:success)

 C/C++:
  int talshShutdown( //out: error code (0:success)
  )

API: Get a flat device id:

 Fortran 2003:
  function encode_device_id(dev_kind,dev_num) result(dev_id):
   integer(C_INT):: dev_id                      !out: flat device id (Success: [0..DEV_MAX-1])
   integer(C_INT), intent(in), value:: dev_kind !in: device kind (see above)
   integer(C_INT), intent(in), value:: dev_num  !in: device number within its kind [0..max]

 C/C++:
  int encode_device_id( //out: flat device id (Success: [0..DEV_MAX-1])
   int dev_kind,        //in: device kind (see above)
   int dev_num          //in: device number within its kind [0..max]
  )

  `Note: This function will be renamed: "talsh_flat_dev_id".

API: Convert a flat device id into the kind specific device id:

 Fortran 2003:
  function decode_device_id(dev_id,dev_kind) result(dev_num)
   integer(C_INT):: dev_num                   !out: kind specific device number [0..max]
   integer(C_INT), intent(in), value:: dev_id !in: flat device id
   integer(C_INT), intent(out):: dev_kind     !out: device kind (see above)

 C/C++:
  int decode_device_id( //out: kind specific device number [0..max]
   int dev_id,          //in: flat device id
   int * dev_kind       //out: device kind (see above)
  )

 `Note: This function will be renamed: "talsh_kind_dev_id".

API: Query the state of a device:

 Fortran 2003:
  function talsh_device_state(dev_num,dev_kind) result(dev_state)
   integer(C_INT):: dev_state                             !out: device state (Success:[DEV_OFF,DEV_ON,DEV_ON_BLAS])
   integer(C_INT), intent(in), value:: dev_num            !in: either a flat or kind specific (when <dev_kind> is present) device id
   integer(C_INT), intent(in), value, optional:: dev_kind !in: device kind (note that it changes the meaning of the <dev_num> argument)

 C/C++:
  int talshDeviceState(  //out: device state (Success:[DEV_OFF,DEV_ON,DEV_ON_BLAS])
   int dev_num,          //in: either a flat or kind specific (when <dev_kind> is present) device id
   int dev_kind=DEV_NULL //in: device kind (note that it changes the meaning of the <dev_num> argument)
  )

 Description:
  DEV_OFF: Device is off (cannot be used by TAL-SH);
  DEV_ON: Device is on (can be used by TAL-SH);
  DEV_ON_BLAS: Device is on and can use vendor provided BLAS.

API: Find the least busy device:

 Fortran 2003:
  function talsh_device_busy_least(dev_kind) result(dev_id)
   integer(C_INT):: dev_id                                !out: either a flat or kind specific device id
   integer(C_INT), intent(in), value, optional:: dev_kind !in: device kind (if absent, <dev_id> will return the flat device id)

 C/C++:
  int talshDeviceBusyLeast( //out: either a flat or kind specific device id
   int dev_kind=DEV_NULL    //in: device kind (if absent, the flat device id will be returned)
  )

API: Print TAL-SH statistics:

 Fortran 2003:
  function talsh_stats(dev_id,dev_kind) result(ierr)
   integer(C_INT):: ierr                           !out: error code (0:success)
   integer(C_INT), intent(in), optional:: dev_id   !in: device id (either flat or kind specific device id, see below)
   integer(C_INT), intent(in), optional:: dev_kind !in: device kind (if present, <dev_id> will be interpreted as kind specific)

 C/C++:
  int talshStats(        //out: error code (0:success)
   int dev_id=-1,        //in: device id (either flat or kind specific device id, see below)
   int dev_kind=DEV_NULL //in: device kind (if present, <dev_id> will be interpreted as kind specific)
  )

 Description:
  If no arguments are passed, the statistics will be shown for all active devices.
  For each device, the statistics includes:
   (a) Time active (sec);
   (b) Number of TAL-SH tasks submitted;
   (c) Number of TAL-SH tasks completed;
   (d) Number of TAL-SH tasks deferred (TRY_LATER);
   (e) Number of TAL-SH tasks failed (except TRY_LATER);
   (f) Number of Flops processed (successful only);
   (g) Input data transferred (bytes);
   (h) Output data transferred (bytes);


3. Tensor block API

At the user level, a tensor block is represented by an interoperable type <talsh_tens_t>.
A tensor block either can be constructed from scrath, including memory allocation, or
an existing tensor block can be registered for the use in TAL-SH. Furthermore, an allocated
tensor block can be initialized to some constant value or a user-defined initialization method
can be invoked on it. In all cases, TAL-SH does not allocate/deallocate the <talsh_tens_t> object
itself, which is the user responsibility! Instead TAL-SH uses an exsiting <talsh_tens_t> handle.

For developers only:
 Depending on the device, a tensor block is specified via one of the two internal types:
 <tensor_block_t> (for the use on CPU and MIC) and <tensBlck_t> (for the use on NVidia GPU).
 Type <tensor_block_t> is natively defined in Fortran 2003 and is emulated in C (opaque handle).
 Type <tensBlck_t> is natively defined in C and is emulated in Fortran (opaque handle).

API: Construct a tensor block:

 Fortran 2003:
  function talsh_tensor_construct(tens_block,data_type,tens_shape,ext_ptr,use_hab,dev_target,init_val,init_method) result(ierr)
   integer(C_INT):: ierr !out: error code (0: success, TRY_LATER: currently no enough memory available)
   type(talsh_tens_t), intent(inout):: tens_block                      !out: constructed tensor block
   integer(C_INT), intent(in):: data_type                              !in: data type: {R4,R8,C8}
   {integer(C_INT),dimension(:)|character(*)|type(tensor_shape_t)}, intent(in):: tens_shape !in: tensor shape (either digital or symbolic)
   type(C_PTR), intent(in), optional:: ext_ptr                         !in: pointer to an externally provided memory for tensor elements
   logical, intent(in), optional:: use_hab                             !in: if TRUE, memory allocation will be done via HAB (defaults to FALSE)
   complex(8), intent(in), optional:: init_val                         !in: initialization value (will be typecast to <data_type>, defaults to 0)
   procedure(talsh_tens_init_i), optional:: init_method                !in: user-defined initialization method

 C/C++:
  int talshTensorConstruct(    //out: error code (0: success, TRY_LATER: currently no enough memory available)
   talsh_tens_t * tens_block,  //out: constructed tensor block
   int data_type,              //in: data type: {R4,R8,C8}
   int tens_rank,              //in: tensor block rank
   int tens_dims[],            //in: tensor block dimension extents
   void * ext_ptr = NULL,      //in: pointer to an externally provided memory for tensor elements
   int use_hab = 0,            //in: if TRUE, memory allocation will be done via HAB (defaults to FALSE)
   double init_val_real = 0.0  //in: initialization value (real part), defaults to 0
   double init_val_imag = 0.0  //in: initialization value (imaginary part), defaults to 0
   talsh_tens_init_i = NULL    //in: user-defined initialization method (function pointer)
  )

 Description:
  A tensor block shape can be specified via an integer array whose length
  will define the tensor rank and each element will define the extent of the
  corresponding dimension of the tensor block. Alternatively, the shape of a
  tensor block can be specified symbolically as a string of characters:
  '(E1,E2,...,En)',
  where
   n is the tensor block rank (the number of dimensions),
   Ex is the extent of dimension x.
  By default, the seniority of tensor dimensions increases to the right (Fortran-like).
  An optional user-defined initialization method must have the following interface:
  Fortran 2003:
   abstract interface
    subroutine talsh_tens_init_i(tens_ptr,data_type,tens_rank,tens_dims,ierr)
     type(C_PTR), value:: tens_ptr                !in: pointer to the tensor elements storage
     integer(C_INT), value:: data_type            !in: data type: {R4,R8,C8}
     integer(C_INT), value:: tens_rank            !in: tensor block rank
     integer(C_INT), intent(in):: tens_dims(1:*)  !in: tensor block dimension extents
     integer(C_INT), intent(out):: ierr           !out: error code (0:success)
    end subroutine talsh_tens_init_i
   end interface
  C/C++:
   typedef void (*talsh_tens_init_i)(void * tens_ptr, int data_type, int tens_rank, int tens_dims[], int * ierr);

API: Destroy a tensor block:

 Fortran 2003:
  function talsh_tensor_destroy(tens_block) result(ierr)
   integer(C_INT):: ierr                          !out: error code (0:success)
   type(talsh_tens_t), intent(inout):: tens_block !inout: destroyed tensor block (but not <tens_block> itself)

 C/C++:
  int talshTensorDestroy(    //out: error code (0:success)
   talsh_tens_t * tens_block //inout: destroyed tensor block (but not <tens_block> itself)
  )

API: Get the volume of a tensor block (number of elements):

 Fortran 2003:
  function talsh_tensor_volume(tens_block) result(vol)
   integer(INTL):: vol                         !out: number of elements in the tensor block (negative on error)
   type(talsh_tens_t), intent(in):: tens_block !in: tensor block

 C/C++:
  long long int talshTensorVolume( //out: number of elements in the tensor block (negative on error)
   const talsh_tens_t * tens_block //in: pointer to a tensor block
  )

 Note: Return value 0 means that the tensor block is empty (null).

API: Get the data type of the tensor block:

 Fortran 2003:
  function talsh_tensor_datatype(tens_block) result(datatyp)
   integer(C_INT):: datatyp                    !out: tensor data type (Success:{R4,R8,C8}; Error:NO_TYPE}
   type(talsh_tens_t), intent(in):: tens_block !in: tensor block

 C/C++:
  int talshTensorDatatype(         //out: tensor data type (Success:{R4,R8,C8}; Error:NO_TYPE)
   const talsh_tens_t * tens_block //in: pointer to a tensor block
  )

API: Get the shape of a tensor block:

 Fortran 2003:
  function talsh_tensor_shape(tens_block,tens_shape) result(ierr)
   integer(C_INT):: ierr                            !out: error code (0:success)
   type(talsh_tens_t), intent(in):: tens_block      !in: tensor block
   type(tensor_shape_t), intent(inout):: tens_shape !out: tensor block shape

 C/C++:
  int talshTensorShape(             //out: error code (0:success)
   const talsh_tens_t * tens_block, //in: pointer to a tensor block
   talsh_tens_shape_t * tens_shape  //out: pointer to the shape of the tensor block (copy)
  )

API: Query the presence of a tensor block on device(s):

 Fortran 2003:
  function talsh_tensor_presence(tens_block,ncopies,copies,dev_kind,dev_id) result(ierr)
   integer(C_INT):: ierr                           !out: error code (0:success)
   type(talsh_tens_t), intent(in):: tens_block     !in: tensor block
   integer(C_INT), intent(out):: ncopies           !out: number of copies of the tensor block found
   integer(C_INT), intent(inout):: copies(1:*)     !out: copies found (list of device id's)
   integer(C_INT), intent(in), optional:: dev_kind !in: specific device kind of interest (defaults to All)
   integer(C_INT), intent(in), optional:: dev_id   !in: specific device of interest

 C/C++:
  int talshTensorPresence(          //out: error code (0:success)
   const talsh_tens_t * tens_block, //in: pointer to a tensor block
   int * ncopies,                   //out: number of copies of the tensor block found
   int copies[],                    //out: copies found (list of device id's)
   int dev_kind = DEV_NULL,         //in: specific device kind of interest (defaults to All)
   int dev_id = -1                  //in: specific device of interest
  )

 Note:
  This function returns a list of device id's which contain the given tensor block in memory.
  If <dev_kind> is present, <dev_id> (if present) carries the kind specific device id.
  If <dev_kind> is absent, <dev_id> (if present) carries the flat device id.


4. TAL-SH task API

A TAL-SH task handle <talsh_task_t> is returned by every asynchronous tensor
operation. It is associated with that specific operation and can be used for
checking the completion of the operation and obtaining the timing information.

API: Clean a TAL-SH task for reuse:

 Fortran 2003:
  function talsh_task_clean(task) result(ierr)
   integer(C_INT):: ierr                    !out: error code (0:success)
   type(talsh_task_t), intent(inout):: task !inout: TAL-SH task handle

 C/C++:
  int talshTaskClean(  //out: error code (0:success)
   talsh_task_t * task //inout: TAL-SH task handle
  )

 Note: A TAL-SH task handle needs to be cleaned before it can be used in another tensor operation.
       Also, the TAL-SH task handle must be cleaned before its first use.

API: Get the id of the device which the task is scheduled on:

 Fortran 2003:
  function talsh_task_dev_id(task,dev_kind) result(dev_id)
   integer(C_INT):: dev_id                          !out: device id (either kind-specific or flat), -1 if error
   type(talsh_task_t), intent(in):: task            !in: TAL-SH task handle
   integer(C_INT), intent(out), optional:: dev_kind !out: device kind (if present, <dev_id> will return the kind-specific device id)

 C/C++:
  int talshTaskDevId(          //out: device id (either kind-specific or flat), -1 if error
   const talsh_task_t * task,  //in: TAL-SH task handle
   int * dev_kind = NULL       //out: device kind (if present, <dev_id> will return the kind-specific device id)
  )

API: Get the TAL-SH task status:

 Fortran 2003:
  function talsh_task_status(task) result(stat)
   integer(C_INT):: stat             !out: task status (see below), TALSH_FAILURE if failed
   type(talsh_task_t), intent(inout) !inout: TAL-SH task handle

 C/C++:
  int talshTaskStatus( //out: task status (see below), TALSH_FAILURE if failed
   talsh_task_t * stat //inout: TAL-SH task handle
  )

 Description:
  Possible task status:
   TALSH_TASK_ERROR: An error occured during task scheduling or execution
   TALSH_TASK_EMPTY: The task has not been scheduled yet
   TALSH_TASK_SCHEDULED: The task has been successfully scheduled
   TALSH_TASK_STARTED: The task started execution
   TALSH_TASK_INPUT_READY: The input data is on device
   TALSH_TASK_OUTPUT_READY: The output data is on device
   TALSH_TASK_COMPLETED: The task is completed

  Note:
   If the task scheduling call (tensor operation) returns TRY_LATER or DEVICE_UNSUITABLE,
   which are not errors, the task status is kept TALSH_TASK_EMPTY.

API: Check whether a TAL-SH task has completed:

 Fortran 2003:
  function talsh_task_completed(task,ierr) result(done)
   logical:: done                           !out: TRUE if the task has completed, FALSE otherwise
   type(talsh_task_t), intent(inout):: task !inout: TAL-SH task handle
   integer(C_INT), intent(out):: ierr       !out: error code (0:success)

 C/C++:
  int talshTaskCompleted( //out: 1 if the task has completed, 0 otherwise
   talsh_task_t * task,   //inout: TAL-SH task handle
   int * ierr             //out: error code (0:success)
  )

API: Wait upon a completion of a TAL-SH task:

 Fortran 2003:
  talsh_task_wait(task) result(ierr)
   integer(C_INT):: ierr                    !out: error code (0:success)
   type(talsh_task_t), intent(inout):: task !inout: TAL-SH task handle

 C/C++:
  int talshTaskWait(   //out: error code (0:success)
   talsh_task_t * task //inout: TAL-SH task handle
  )

API: Wait upon a completion of multiple TAL-SH tasks:

 Fortran 2003:
  function talsh_tasks_wait(ntasks,tasks) result(ierr)
   integer(C_INT):: ierr                               !out: error code (0:success)
   integer(C_INT), intent(in):: ntasks                 !in: number of tasks
   type(talsh_task_t), intent(inout):: tasks(1:ntasks) !inout: TAL-SH tasks handles

 C/C++:
  int talshTasksWait(    //out: error code (0:success)
   int ntasks,           //in: number of tasks
   talsh_task_t *tasks[] //inout: TAL-SH tasks handles
  )

API: Get the task timings:

 Fortran 2003:
  function talsh_task_time(task,total,comput,input,output) result(ierr)
   integer(C_INT):: ierr                   !out: error code (0:success)
   type(talsh_task_t), intent(in):: task   !in: TAL-SH task handle
   real(8), intent(out):: total            !out: total execution time (sec)
   real(8), intent(out), optional:: comput !out: time the computation took (sec)
   real(8), intent(out), optional:: input  !out: time the ingoing data transfers took (sec)
   real(8), intent(out), optional:: output !out: time the outgoing data transfers took (sec)

 C/C++:
  int talshTaskTime(          //out: error code (0:success)
   const talsh_task_t * task, //in: TAL-SH task handle
   double * total,            //out: total execution time (sec)
   double * comput = NULL,    //out: time the computation took (sec)
   double * input = NULL,     //out: time the ingoing data transfers took (sec)
   double * output = NULL     //out: time the outgoing data transfers took (sec)
  )

 Note:
  Since an asynchronous execution of multiple tasks on the same device utilizes
  the same hardware resources, the corresponding timings will be non-exclusive.


5. Tensor operations

Each tensor operation can be executed asynchronously with respect to the Host,
unless a synchronization is requested. An asynchronous execution is activated
automatically when a TAL-SH task handle passed to the corresponding call. Also,
each tensor operation accepts an argument, called "copy control", that regulates
whether the data will be kept on device or discarded after the task completion.
It also specifies whether the data should be discarded from the source device after
it has been copied to the device which will execute the task (if those are distinct).
In the case when a tensor block resides on one device but participates in a tensor
operation scheduled on a different device, the former will be called the "source device"
while the latter will be called the "destination device" (the two can be the same in general).
The "copy control" parameter name has the following structure:
 COPY_X: For tensor operations with a single tensor argument;
 COPY_XX: For tensor operations with two tensor arguments (lhs,rhs);
 COPY_XXX: For tensor operations with three tensor arguments (lhs,rhs1,rhs2).
where each "X" refers to the corresponding tensor argument (tensor block),
with the following options:
 X=D: Discard the tensor argument from both the source and destination devices after the task completion;
 X=M: Discard the tensor argument from the source device after the task completion if the source
      device differs from the destination device, otherwise do nothing;
 X=T: Discard the tensor argument from the destination device if the destination device
      differs from the source device, otherwise do nothing;
 X=K: Keep the tensor argument on both devices if they are different.

If a task cannot be scheduled at this point due to the lack of resources (memory, etc.),
a return status TRY_LATER will indicate that. If a task cannot be scheduled
in principle on a particular device, a return status DEVICE_UNABLE will
indicate that. Note that both TRY_LATER and DEVICE_UNABLE are not errors.
A successful scheduling will be indicated by a return status TALSH_SUCCESS (0).
A successful scheduling does not guarantee a successful execution, in general.

API: Place a tensor block on a specific device:

 Fortran 2003:
  function talsh_tensor_place(tens,dev_id,dev_kind,copy_ctrl,task) result(ierr)
   integer(C_INT):: ierr                              !out: error code (0:success)
   type(talsh_tens_t), intent(inout):: tens           !inout: tensor block
   integer(C_INT), intent(in):: dev_id                !in: device id (flat or kind-specific)
   integer(C_INT), intent(in), optional:: dev_kind    !in: device kind (if present, <dev_id> is kind-specific)
   integer(C_INT), intent(in), optional:: copy_ctrl   !in: copy control (COPY_X), defaults to COPY_M
   type(talsh_task_t), intent(inout), optional:: task !inout: TAL-SH task handle

 C/C++:
  int talshTensorPlace(       //out: error code (0:success)
   talsh_tens_t * tens,       //inout: tensor block
   int dev_id,                //in: device id (flat or kind-specific)
   int dev_kind = DEV_NULL,   //in: device kind (if present, <dev_id> is kind-specific)
   int copy_ctrl = COPY_M,    //in: copy control (COPY_X), defaults to COPY_M
   talsh_task_t * task = NULL //inout: TAL-SH task handle
  )

API: Discard a tensor block from a specific device:

 Fortran 2003:
  function talsh_tensor_discard(tens,dev_id,dev_kind) result(ierr)
   integer(C_INT):: ierr                           !out: error code (0:success)
   type(talsh_tens_t), intent(inout):: tens        !inout: tensor block
   integer(C_INT), intent(in):: dev_id             !in: device id (flat or kind-specific)
   integer(C_INT), intent(in), optional:: dev_kind !in: device kind (if present, <dev_id> is kind-specific)

 C/C++:
  int talshTensorDiscard(  //out: error code (0:success)
   talsh_tens_t * tens,    //inout: tensor block
   int dev_id,             //in: device id (flat or kind-specific)
   int dev_kind = DEV_NULL //in: device kind (if present, <dev_id> is kind-specific)
  )

 Description:
  Resources occupied by a tensor block on the given device, if any, will be released,
  making the tensor block "absent" on that device.

API: Initialize a tensor block:

 Fortran 2003:
  function talsh_tensor_init(tens,val,dev_id,dev_kind,copy_ctrl,task) result(ierr)
   integer(C_INT):: ierr                              !out: error code (0:success)
   type(talsh_tens_t), intent(inout):: tens           !inout: tensor block
   complex(8), intent(in):: val                       !in: initialization value
   integer(C_INT), intent(in), optional:: dev_id      !in: device id (flat or kind-specific)
   integer(C_INT), intent(in), optional:: dev_kind    !in: device kind (if present, <dev_id> is kind-specific)
   integer(C_INT), intent(in), optional:: copy_ctrl   !in: copy control (COPY_X), defaults to COPY_M
   type(talsh_task_t), intent(inout), optional:: task !inout: TAL-SH task handle

 C/C++:
  int talshTensorInit(        //out: error code (0:success)
   talsh_tens_t * tens,       //inout: tensor block
   double val_real,           //in: initialization value (real part)
   double val_imag,           //in: initialization value (complex part)
   int dev_id = -1,           //in: device id (flat or kind-specific)
   int dev_kind = DEV_NULL,   //in: device kind (if present, <dev_id> is kind-specific)
   int copy_ctrl = COPY_M,    //in: copy control (COPY_X), defaults to COPY_M
   talsh_task_t * task = NULL //inout: TAL-SH task handle
  )

 Note: If no device is specified, the execution device will be automatically
       selected from the presence list of the tensor block.

API: Multiply a tensor block by a scalar:

 Fortran 2003:
  function talsh_tensor_scale(tens,val,dev_id,dev_kind,copy_ctrl,task) result(ierr)
   integer(C_INT):: ierr                              !out: error code (0:success)
   type(talsh_tens_t), intent(inout):: tens           !inout: tensor block
   complex(8), intent(in):: val                       !in: scaling value
   integer(C_INT), intent(in), optional:: dev_id      !in: device id (flat or kind-specific)
   integer(C_INT), intent(in), optional:: dev_kind    !in: device kind (if present, <dev_id> is kind-specific)
   integer(C_INT), intent(in), optional:: copy_ctrl   !in: copy control (COPY_X), defaults to COPY_M
   type(talsh_task_t), intent(inout), optional:: task !inout: TAL-SH task handle

 C/C++:
  int talshTensorScale(       //out: error code (0:success)
   talsh_tens_t * tens,       //inout: tensor block
   double val_real,           //in: scaling value (real part)
   double val_imag,           //in: scaling value (complex part)
   int dev_id = -1,           //in: device id (flat or kind-specific)
   int dev_kind = DEV_NULL,   //in: device kind (if present, <dev_id> is kind-specific)
   int copy_ctrl = COPY_M,    //in: copy control (COPY_X), defaults to COPY_M
   talsh_task_t * task = NULL //inout: TAL-SH task handle
  )

 Note: If no device is specified, the execution device will be automatically
       selected from the presence list of the tensor block.

API: Compute the 1-norm of a tensor block:

 Fortran 2003:
  function talsh_tensor_norm1(dtens,ltens,dev_id,dev_kind,copy_ctrl,task) result(ierr)
   integer(C_INT):: ierr                              !out: error code (0:success)
   type(talsh_tens_t), intent(inout):: dtens          !inout: rank-0 destination tensor (scalar containing the 1-norm)
   type(talsh_tens_t), intent(inout):: ltens          !inout: tensor block
   integer(C_INT), intent(in), optional:: dev_id      !in: device id (flat or kind-specific)
   integer(C_INT), intent(in), optional:: dev_kind    !in: device kind (if present, <dev_id> is kind-specific)
   integer(C_INT), intent(in), optional:: copy_ctrl   !in: copy control (COPY_XX), defaults to COPY_TT
   type(talsh_task_t), intent(inout), optional:: task !inout: TAL-SH task handle

 C/C++:
  int talshTensorNorm1(       //out: error code (0:success)
   talsh_tens_t * dtens,      //inout: rank-0 destination tensor (scalar containing the 1-norm)
   talsh_tens_t * ltens,      //inout: tensor block
   int dev_id = -1,           //in: device id (flat or kind-specific)
   int dev_kind = DEV_NULL,   //in: device kind (if present, <dev_id> is kind-specific)
   int copy_ctrl = COPY_TT,   //in: copy control (COPY_XX), defaults to COPY_TT
   talsh_task_t * task = NULL //inout: TAL-SH task handle
  )

 Note: If no device is specified, the execution device will be automatically
       selected from the presence list of the tensor block.

API: Compute the 2-norm of a tensor block:

 Fortran 2003:
  function talsh_tensor_norm2(dtens,ltens,dev_id,dev_kind,copy_ctrl,task) result(ierr)
   integer(C_INT):: ierr                              !out: error code (0:success)
   type(talsh_tens_t), intent(inout):: dtens          !inout: rank-0 destination tensor (scalar containing the 2-norm)
   type(talsh_tens_t), intent(inout):: ltens          !inout: tensor block
   integer(C_INT), intent(in), optional:: dev_id      !in: device id (flat or kind-specific)
   integer(C_INT), intent(in), optional:: dev_kind    !in: device kind (if present, <dev_id> is kind-specific)
   integer(C_INT), intent(in), optional:: copy_ctrl   !in: copy control (COPY_XX), defaults to COPY_TT
   type(talsh_task_t), intent(inout), optional:: task !inout: TAL-SH task handle

 C/C++:
  int talshTensorNorm2(       //out: error code (0:success)
   talsh_tens_t * dtens,      //inout: rank-0 destination tensor (scalar containing the 2-norm)
   talsh_tens_t * ltens,      //inout: tensor block
   int dev_id = -1,           //in: device id (flat or kind-specific)
   int dev_kind = DEV_NULL,   //in: device kind (if present, <dev_id> is kind-specific)
   int copy_ctrl = COPY_TT,   //in: copy control (COPY_XX), defaults to COPY_TT
   talsh_task_t * task = NULL //inout: TAL-SH task handle
  )

 Note: If no device is specified, the execution device will be automatically
       selected from the presence list of the tensor block.

API: Copy a tensor block with an optional dimension reordering:

 Fortran 2003:
  function talsh_tensor_copy(dtens,ltens,prmn,scale,dev_id,dev_kind,copy_ctrl,task) result(ierr)
   integer(C_INT):: ierr                              !out: error code (0:success)
   type(talsh_tens_t), intent(inout):: dtens          !inout: destination tensor block
   type(talsh_tens_t), intent(inout):: ltens          !inout: source tensor block
   integer(C_INT), intent(in), optional:: prmn(1:)    !in: dimension permutation, defaults to trivial
   complex(8), intent(in), optional:: scale           !in: scaling value, defaults to 1
   integer(C_INT), intent(in), optional:: dev_id      !in: device id (flat or kind-specific)
   integer(C_INT), intent(in), optional:: dev_kind    !in: device kind (if present, <dev_id> is kind-specific)
   integer(C_INT), intent(in), optional:: copy_ctrl   !in: copy control (COPY_XX), defaults to COPY_MT
   type(talsh_task_t), intent(inout), optional:: task !inout: TAL-SH task handle

 C/C++:
  int talshTensorCopy(         //out: error code (0:success)
   talsh_tens_t * dtens,       //inout: destination tensor block
   talsh_tens_t * ltens,       //inout: source tensor block
   int * prmn = NULL,          //in: dimension permutation, defaults to trivial
   double scale_real = 1.0,    //in: scaling value (real part), defaults to 1
   double scale_imag = 0.0,    //in: scaling value (imaginary part), defaults to 0
   int dev_id = -1,            //in: device id (flat or kind-specific)
   int dev_kind = DEV_NULL,    //in: device kind (if present, <dev_id> is kind-specific)
   int copy_ctrl = COPY_MT,    //in: copy control (COPY_XX), defaults to COPY_MT
   talsh_task_t * task = NULL, //inout: TAL-SH task handle
  )

 Note: If no device is specified, the execution device will be automatically
       selected from the presence lists of the tensor blocks.

API: Add a tensor block to another tensor block:

 Fortran 2003:
  function talsh_tensor_add(dtens,ltens,scale,dev_id,dev_kind,copy_ctrl,task) result(ierr)
   integer(C_INT):: ierr                              !out: error code (0:success)
   type(talsh_tens_t), intent(inout):: dtens          !inout: destination tensor block
   type(talsh_tens_t), intent(inout):: ltens          !inout: source tensor block
   complex(8), intent(in), optional:: scale           !in: scaling value, defaults to 1
   integer(C_INT), intent(in), optional:: dev_id      !in: device id (flat or kind-specific)
   integer(C_INT), intent(in), optional:: dev_kind    !in: device kind (if present, <dev_id> is kind-specific)
   integer(C_INT), intent(in), optional:: copy_ctrl   !in: copy control (COPY_XX), defaults to COPY_MT
   type(talsh_task_t), intent(inout), optional:: task !inout: TAL-SH task handle

 C/C++:
  int talshTensorAdd(          //out: error code (0:success)
   talsh_tens_t * dtens,       //inout: destination tensor block
   talsh_tens_t * ltens,       //inout: source tensor block
   double scale_real = 1.0,    //in: scaling value (real part), defaults to 1
   double scale_imag = 0.0,    //in: scaling value (imaginary part), defaults to 0
   int dev_id = -1,            //in: device id (flat or kind-specific)
   int dev_kind = DEV_NULL,    //in: device kind (if present, <dev_id> is kind-specific)
   int copy_ctrl = COPY_MT,    //in: copy control (COPY_XX), defaults to COPY_MT
   talsh_task_t * task = NULL, //inout: TAL-SH task handle
  )

 Note: If no device is specified, the execution device will be automatically
       selected from the presence lists of the tensor blocks.

API: Contract two tensor blocks and add the result to another tensor block:

 Fortran 2003:
  function talsh_tensor_contract(cptrn,dtens,ltens,rtens,scale,dev_id,dev_kind,copy_ctrl,task) result(ierr)
   integer(C_INT):: ierr                              !out: error code (0:success)
   character(*), intent(in):: cptrn                   !in: symbolic contraction pattern, e.g. "D(a,b,c,d)+=L(c,i,j,a)*R(b,j,d,i)"
   type(talsh_tens_t), intent(inout):: dtens          !inout: destination tensor block
   type(talsh_tens_t), intent(inout):: ltens          !inout: left source tensor block
   type(talsh_tens_t), intent(inout):: rtens          !inout: right source tensor block
   complex(8), intent(in), optional:: scale           !in: scaling value, defaults to 1
   integer(C_INT), intent(in), optional:: dev_id      !in: device id (flat or kind-specific)
   integer(C_INT), intent(in), optional:: dev_kind    !in: device kind (if present, <dev_id> is kind-specific)
   integer(C_INT), intent(in), optional:: copy_ctrl   !in: copy control (COPY_XXX), defaults to COPY_MTT
   type(talsh_task_t), intent(inout), optional:: task !inout: TAL-SH task handle

 C/C++:
  int talshTensorContract(     //out: error code (0:success)
   char * cptrn,               //in: C-string: symbolic contraction pattern, e.g. "D(a,b,c,d)+=L(c,i,j,a)*R(b,j,d,i)"
   talsh_tens_t * dtens,       //inout: destination tensor block
   talsh_tens_t * ltens,       //inout: left source tensor block
   talsh_tens_t * rtens,       //inout: right source tensor block
   double scale_real = 1.0,    //in: scaling value (real part), defaults to 1
   double scale_imag = 0.0,    //in: scaling value (imaginary part), defaults to 0
   int dev_id = -1,            //in: device id (flat or kind-specific)
   int dev_kind = DEV_NULL,    //in: device kind (if present, <dev_id> is kind-specific)
   int copy_ctrl = COPY_MTT,   //in: copy control (COPY_XXX), defaults to COPY_MTT
   talsh_task_t * task = NULL, //inout: TAL-SH task handle
  )

 Note: If no device is specified, the execution device will be automatically
       selected from the presence lists of the tensor blocks.
